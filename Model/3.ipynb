{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71b7306fcca46f3bfeb9df09689bc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\WSH3\\AppData\\Local\\Temp\\5\\ipykernel_4336\\3351557976.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(X_train_text), torch.tensor(X_train_num, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.long))\n",
      "C:\\Users\\WSH3\\AppData\\Local\\Temp\\5\\ipykernel_4336\\3351557976.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(X_test_text), torch.tensor(X_test_num, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.long))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49c0f40553b412aa7979b2ee41e9582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/6:   0%|          | 0/21024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(texts, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     77\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m accumulation_steps\n\u001b[1;32m---> 79\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader):\n\u001b[0;32m     82\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:181\u001b[0m, in \u001b[0;36mGradScaler.scale\u001b[1;34m(self, outputs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Short-circuit for the common case.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init_scale_growth_tracker(outputs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Check if the tokenizer has a padding token, if not, set one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"../Dataset_with_Features/dataset_420464.csv\")\n",
    "\n",
    "# Separate features\n",
    "text_data = data[\"url\"]\n",
    "numerical_data = data.drop(columns=[\"url\", \"label\"])\n",
    "\n",
    "# Preprocess numerical data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Tokenize text data\n",
    "max_length = 128\n",
    "tokenized_data = tokenizer(text_data.tolist(), padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "X_text = tokenized_data[\"input_ids\"]\n",
    "\n",
    "# Prepare labels\n",
    "y = data[\"label\"].apply(lambda x: 1 if x == \"bad\" else 0)\n",
    "\n",
    "# Split dataset\n",
    "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(X_text, X_num, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_text), torch.tensor(X_train_num, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_text), torch.tensor(X_test_num, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "# DataLoader setup\n",
    "batch_size = 16  # Reduced batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Training settings\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 6\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Initialize the gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient Accumulation Settings\n",
    "accumulation_steps = 4\n",
    "\n",
    "# Training loop with mixed precision and gradient accumulation\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        texts, nums, labels = batch\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(texts, labels=labels)\n",
    "            loss = outputs.loss / accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {average_loss}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    for batch in test_loader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        texts, nums, labels = batch\n",
    "\n",
    "        with torch.no_grad(), autocast():\n",
    "            outputs = model(texts, labels=labels)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (predictions == labels).cpu().numpy().mean()\n",
    "        total_eval_accuracy += accuracy\n",
    "\n",
    "    average_accuracy = total_eval_accuracy / len(test_loader)\n",
    "    print(f\"Accuracy: {average_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "max_length = 128  # or another appropriate value based on your data\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Ensure tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('../Dataset_with_Features/dataset_420464.csv')\n",
    "text_data = data['url']\n",
    "numerical_data = data.drop(columns=['url', 'label'])\n",
    "y = data['label'].apply(lambda x: 1 if x == 'bad' else 0)\n",
    "\n",
    "# Preprocess numerical data\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Tokenize text data\n",
    "tokenized_data = tokenizer(text_data.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "X_text = tokenized_data['input_ids']\n",
    "\n",
    "# Split dataset\n",
    "X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = train_test_split(X_text, X_num, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_text, torch.tensor(X_train_num, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.long))\n",
    "test_dataset = TensorDataset(X_test_text, torch.tensor(X_test_num, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1ff0856ad049dc938cb1d177fe8d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, transformer_model_name, num_numerical_features, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(transformer_model_name, num_labels=num_labels)\n",
    "        self.numerical_processor = nn.Linear(num_numerical_features, 64)  # Example layer\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size + 64, num_labels)\n",
    "\n",
    "    def forward(self, text, numerical_features, attention_mask=None):\n",
    "        transformer_output = self.transformer(text, attention_mask=attention_mask).logits\n",
    "        numerical_output = self.numerical_processor(numerical_features)\n",
    "        combined = torch.cat((transformer_output, numerical_output), dim=1)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n",
    "\n",
    "# Instantiate model\n",
    "model = CustomModel(\"meta-llama/Llama-2-7b-hf\", X_train_num.shape[1], 2)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 3)  # 3 epochs\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for step, (texts, nums, labels) in progress_bar:\n",
    "        attention_masks = (texts != tokenizer.pad_token_id).long()\n",
    "        texts, attention_masks, nums, labels = texts.to(device), attention_masks.to(device), nums.to(device), labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(texts, nums, attention_masks)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': total_loss / (step + 1)})\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {average_loss}\")\n",
    "\n",
    "# Add validation loop if needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

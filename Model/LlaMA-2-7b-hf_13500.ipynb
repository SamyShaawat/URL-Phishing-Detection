{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7072323993403ca49ef4298135f010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Datasets/dataset_13500.csv\")\n",
    "\n",
    "\n",
    "text_data = data[\"url\"]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "max_length = 128\n",
    "tokenized_data = tokenizer(\n",
    "    text_data.tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "X_text = tokenized_data[\"input_ids\"]\n",
    "\n",
    "\n",
    "y = data[\"label\"].apply(lambda x: 1 if x == \"bad\" else 0)\n",
    "\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    X_train_text, torch.tensor(y_train.values, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(X_test_text, torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 6\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"../Model/savedModels/savedModel_13500\"\n",
    "tokenizer_save_path = \"../Model/savedTokenizers/savedTokenizer_13500\"\n",
    "checkpoint_path = \"../Model/savedCheckPoints/checkpoint_13500.pth\"\n",
    "losses_csv_file = \"../Model/savedLosses/losses_13500.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Checkpoint Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scheduler, training_loss):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"Training_loss\": training_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    model.save_pretrained(model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "    tokenizer.save_pretrained(tokenizer_save_path)\n",
    "    print(f\"Tokenizer saved at {tokenizer_save_path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler):\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "\n",
    "        training_loss = checkpoint.get(\"Training_loss\", None)\n",
    "\n",
    "        return checkpoint[\"epoch\"], training_loss\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")\n",
    "        return 0, None\n",
    "\n",
    "\n",
    "def append_loss_to_csv(epoch, train_loss, file_path):\n",
    "    df = pd.DataFrame({\"Epoch\": [epoch], \"Train_Loss\": [train_loss]})\n",
    "\n",
    "    with open(file_path, \"a\") as f:\n",
    "        df.to_csv(f, header=f.tell() == 0, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from ../Model/savedCheckPoints/checkpoint_13500.pth\n"
     ]
    }
   ],
   "source": [
    "start_epoch, training_loss = load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|██████████| 675/675 [16:28:34<00:00, 87.87s/it, loss=0.0444]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/6], Train Loss: 0.0444\n",
      "Checkpoint saved at ../Model/savedCheckPoints/checkpoint_13500.pth\n",
      "Model saved at ../Model/savedModels/savedModel_13500\n",
      "Tokenizer saved at ../Model/savedTokenizers/savedTokenizer_13500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|██████████| 675/675 [18:04:40<00:00, 96.42s/it, loss=0.0241]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/6], Train Loss: 0.0241\n",
      "Checkpoint saved at ../Model/savedCheckPoints/checkpoint_13500.pth\n",
      "Model saved at ../Model/savedModels/savedModel_13500\n",
      "Tokenizer saved at ../Model/savedTokenizers/savedTokenizer_13500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|██████████| 675/675 [16:02:28<00:00, 85.55s/it, loss=0.0114]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/6], Train Loss: 0.0114\n",
      "Checkpoint saved at ../Model/savedCheckPoints/checkpoint_13500.pth\n",
      "Model saved at ../Model/savedModels/savedModel_13500\n",
      "Tokenizer saved at ../Model/savedTokenizers/savedTokenizer_13500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|██████████| 675/675 [15:57:57<00:00, 85.15s/it, loss=0.00125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/6], Train Loss: 0.0013\n",
      "Checkpoint saved at ../Model/savedCheckPoints/checkpoint_13500.pth\n",
      "Model saved at ../Model/savedModels/savedModel_13500\n",
      "Tokenizer saved at ../Model/savedTokenizers/savedTokenizer_13500\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "\n",
    "if training_loss is not None:\n",
    "    train_losses.append(training_loss)\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "    )\n",
    "\n",
    "    for step, (texts, labels) in progress_bar:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(texts, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": total_train_loss / (step + 1)})\n",
    "\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(average_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {average_train_loss:.4f}\")\n",
    "    save_checkpoint(epoch + 1, model, optimizer, scheduler, average_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9782\n",
      "Precision: 0.9719\n",
      "Recall: 0.9710\n",
      "F1 Score: 0.9714\n",
      "   Accuracy  Precision    Recall  F1 Score\n",
      "0  0.978156   0.971899  0.970958  0.971429\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(test_loader, desc=\"Evaluating\", leave=False):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(texts, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average=\"binary\"\n",
    "    )\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "metrics_df = pd.DataFrame(\n",
    "    [[accuracy, precision, recall, f1]],\n",
    "    columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    ")\n",
    "print(metrics_df)\n",
    "metrics_df.to_csv(\"../Model/savedMetrices/Evaluation_dataset_13500_testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|███▋      | 249/675 [1:32:08<3:01:32, 25.57s/it]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in tqdm(train_loader, desc=\"Evaluating\", leave=False):\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(texts, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "        predictions.extend(predicted_labels.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average=\"binary\"\n",
    "    )\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "metrics_df = pd.DataFrame(\n",
    "    [[accuracy, precision, recall, f1]],\n",
    "    columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    ")\n",
    "print(metrics_df)\n",
    "metrics_df.to_csv(\"../Model/savedMetrices/Evaluation_dataset_13500_training.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

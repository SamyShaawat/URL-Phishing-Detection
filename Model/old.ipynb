{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from matplotlib.pyplot import step\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73341b43d514dbca6fd8a632987314e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Update the model configuration to recognize the new padding token ID\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Resize model token embeddings to accommodate new special token\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      url label  use_of_ip  abnormal_url  google_index  \\\n",
      "0  diaryofagameaddict.com   bad          0             0             1   \n",
      "1        espdesign.com.au   bad          0             0             1   \n",
      "2      iamagameaddict.com   bad          0             0             1   \n",
      "3           kalantzis.net   bad          0             0             1   \n",
      "4   slightlyoffcenter.net   bad          0             0             1   \n",
      "\n",
      "   count.  count-www  count@  count_dir  count_embed_domian  ...  count-  \\\n",
      "0       1          0       0          0                   0  ...       0   \n",
      "1       2          0       0          0                   0  ...       0   \n",
      "2       1          0       0          0                   0  ...       0   \n",
      "3       1          0       0          0                   0  ...       0   \n",
      "4       1          0       0          0                   0  ...       0   \n",
      "\n",
      "   count=  url_length  hostname_length  sus_url  count-digits  count-letters  \\\n",
      "0       0          22                0        0             0             21   \n",
      "1       0          16                0        0             0             14   \n",
      "2       0          18                0        0             0             17   \n",
      "3       0          13                0        0             0             12   \n",
      "4       0          21                0        0             0             20   \n",
      "\n",
      "   fd_length  tld  tld_length  \n",
      "0          0  NaN          -1  \n",
      "1          0  NaN          -1  \n",
      "2          0  NaN          -1  \n",
      "3          0  NaN          -1  \n",
      "4          0  NaN          -1  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "           use_of_ip   abnormal_url  google_index         count.  \\\n",
      "count  420464.000000  420464.000000      420464.0  420464.000000   \n",
      "mean        0.007294       0.000316           1.0       1.782614   \n",
      "std         0.085095       0.017783           0.0       1.098021   \n",
      "min         0.000000       0.000000           1.0       0.000000   \n",
      "25%         0.000000       0.000000           1.0       1.000000   \n",
      "50%         0.000000       0.000000           1.0       2.000000   \n",
      "75%         0.000000       0.000000           1.0       2.000000   \n",
      "max         1.000000       1.000000           1.0      32.000000   \n",
      "\n",
      "           count-www         count@      count_dir  count_embed_domian  \\\n",
      "count  420464.000000  420464.000000  420464.000000       420464.000000   \n",
      "mean        0.010874       0.004295       2.373219            0.000371   \n",
      "std         0.105617       0.067474       1.458025            0.021477   \n",
      "min         0.000000       0.000000       0.000000            0.000000   \n",
      "25%         0.000000       0.000000       1.000000            0.000000   \n",
      "50%         0.000000       0.000000       2.000000            0.000000   \n",
      "75%         0.000000       0.000000       3.000000            0.000000   \n",
      "max         5.000000       6.000000      22.000000            5.000000   \n",
      "\n",
      "           short_url    count-https  ...         count?         count-  \\\n",
      "count  420464.000000  420464.000000  ...  420464.000000  420464.000000   \n",
      "mean        0.066277       0.001308  ...       0.146338       1.298054   \n",
      "std         0.248766       0.038440  ...       0.450639       2.632559   \n",
      "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
      "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
      "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
      "75%         0.000000       0.000000  ...       0.000000       1.000000   \n",
      "max         1.000000       4.000000  ...     166.000000      42.000000   \n",
      "\n",
      "              count=     url_length  hostname_length        sus_url  \\\n",
      "count  420464.000000  420464.000000    420464.000000  420464.000000   \n",
      "mean        0.243048      48.342005         0.005943       0.038805   \n",
      "std         0.873145      35.021279         0.385546       0.193130   \n",
      "min         0.000000       1.000000         0.000000       0.000000   \n",
      "25%         0.000000      29.000000         0.000000       0.000000   \n",
      "50%         0.000000      41.000000         0.000000       0.000000   \n",
      "75%         0.000000      58.000000         0.000000       0.000000   \n",
      "max        34.000000    2307.000000        46.000000       1.000000   \n",
      "\n",
      "        count-digits  count-letters      fd_length     tld_length  \n",
      "count  420464.000000  420464.000000  420464.000000  420464.000000  \n",
      "mean        3.945042      37.736891       8.275850      -0.998813  \n",
      "std         8.181256      25.206550      10.248791       0.075211  \n",
      "min         0.000000       0.000000       0.000000      -1.000000  \n",
      "25%         0.000000      24.000000       4.000000      -1.000000  \n",
      "50%         1.000000      33.000000       6.000000      -1.000000  \n",
      "75%         6.000000      46.000000      10.000000      -1.000000  \n",
      "max       545.000000    1835.000000     408.000000       6.000000  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "url                        0\n",
      "label                      0\n",
      "use_of_ip                  0\n",
      "abnormal_url               0\n",
      "google_index               0\n",
      "count.                     0\n",
      "count-www                  0\n",
      "count@                     0\n",
      "count_dir                  0\n",
      "count_embed_domian         0\n",
      "short_url                  0\n",
      "count-https                0\n",
      "count-http                 0\n",
      "count%                     0\n",
      "count?                     0\n",
      "count-                     0\n",
      "count=                     0\n",
      "url_length                 0\n",
      "hostname_length            0\n",
      "sus_url                    0\n",
      "count-digits               0\n",
      "count-letters              0\n",
      "fd_length                  0\n",
      "tld                   420350\n",
      "tld_length                 0\n",
      "dtype: int64\n",
      "label\n",
      "good    344821\n",
      "bad      75643\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('../Dataset_with_Features/dataset_420464.csv', low_memory=False)\n",
    "\n",
    "# Basic data analysis\n",
    "print(data.head())  # View first few rows\n",
    "print(data.describe())  # Statistical summary\n",
    "print(data.isnull().sum())  # Check for missing values\n",
    "print(data['label'].value_counts())  # Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      url  label  use_of_ip  abnormal_url  google_index  \\\n",
      "0  diaryofagameaddict.com      1          0             0             1   \n",
      "1        espdesign.com.au      1          0             0             1   \n",
      "2      iamagameaddict.com      1          0             0             1   \n",
      "3           kalantzis.net      1          0             0             1   \n",
      "4   slightlyoffcenter.net      1          0             0             1   \n",
      "\n",
      "   count.  count-www  count@  count_dir  count_embed_domian  ...  count-  \\\n",
      "0       1          0       0          0                   0  ...       0   \n",
      "1       2          0       0          0                   0  ...       0   \n",
      "2       1          0       0          0                   0  ...       0   \n",
      "3       1          0       0          0                   0  ...       0   \n",
      "4       1          0       0          0                   0  ...       0   \n",
      "\n",
      "   count=  url_length  hostname_length  sus_url  count-digits  count-letters  \\\n",
      "0       0          22                0        0             0             21   \n",
      "1       0          16                0        0             0             14   \n",
      "2       0          18                0        0             0             17   \n",
      "3       0          13                0        0             0             12   \n",
      "4       0          21                0        0             0             20   \n",
      "\n",
      "   fd_length  tld  tld_length  \n",
      "0          0  NaN          -1  \n",
      "1          0  NaN          -1  \n",
      "2          0  NaN          -1  \n",
      "3          0  NaN          -1  \n",
      "4          0  NaN          -1  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "data['label'] = data['label'].map({'good': 0, 'bad': 1})\n",
    "print(data.head())  # View first few rows\n",
    "\n",
    "# Fill missing values\n",
    "data['label'] = data['label'].fillna(data['label'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25)\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|          | 0/252278 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 252278/252278 [01:34<00:00, 2669.62it/s]\n",
      "Tokenizing: 100%|██████████| 84093/84093 [00:33<00:00, 2524.61it/s]\n",
      "Tokenizing: 100%|██████████| 84093/84093 [00:31<00:00, 2668.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_extract_features(data, max_length=512):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for url in tqdm(data['url'].tolist(), desc=\"Tokenizing\"):\n",
    "        encodings = tokenizer(url, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "        input_ids.append(encodings['input_ids'][0])\n",
    "        attention_masks.append(encodings['attention_mask'][0])\n",
    "\n",
    "    feature_columns = [col for col in data.columns if col not in ['url', 'label']]\n",
    "    features = data[feature_columns].values\n",
    "\n",
    "    return input_ids, attention_masks, features, data['label']\n",
    "\n",
    "# Tokenize and extract features\n",
    "train_input_ids, train_attention_masks, train_features, train_labels = tokenize_and_extract_features(train_data)\n",
    "val_input_ids, val_attention_masks, val_features, val_labels = tokenize_and_extract_features(val_data)\n",
    "test_input_ids, test_attention_masks, test_features, test_labels = tokenize_and_extract_features(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, features, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = URLDataset(train_input_ids, train_attention_masks, train_features, train_labels)\n",
    "val_dataset = URLDataset(val_input_ids, val_attention_masks, val_features, val_labels)\n",
    "test_dataset = URLDataset(test_input_ids, test_attention_masks, test_features, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, features, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_masks[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = URLDataset(train_input_ids, train_attention_masks, train_features, train_labels)\n",
    "val_dataset = URLDataset(val_input_ids, val_attention_masks, val_features, val_labels)\n",
    "test_dataset = URLDataset(test_input_ids, test_attention_masks, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "        \n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "        \n",
    "        self.scaler.scale(loss).backward()\n",
    "        \n",
    "        if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()  # Free up GPU memory\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "\n",
    "        with torch.no_grad():  # No gradients computed during evaluation\n",
    "            return super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "    def on_epoch_end(self):\n",
    "        # Identify the current GPU\n",
    "        device = next(self.model.parameters()).device\n",
    "        print(f\"End of epoch. Current device: {device}\")\n",
    "\n",
    "    # Free up GPU memory\n",
    "        gc.collect()  # Collect garbage\n",
    "        torch.cuda.empty_cache()  # Empty cache\n",
    "\n",
    "        super().on_epoch_end()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cpu\n",
      "Current device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(f\"Current device: {device}\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Current device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mCustomTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:456\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[0;32m    455\u001b[0m ):\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:690\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[1;34m(self, model, device)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[1;32m--> 690\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2460\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2459\u001b[0m         )\n\u001b[1;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\WSH3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "print(f\"Current device: {device}\")\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Current device: {device}\")\n",
    "trainer.train()\n",
    "del model, train_dataset, val_dataset, test_dataset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
